{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exploding gradient problem because of big data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, downsampled the data to 24Khz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_hdf(\"/Users/aj/docs/useful/sem3/MLDM project/data & codes/data_24hz_train.h5\")\n",
    "new_df_test = pd.read_hdf(\"/Users/aj/docs/useful/sem3/MLDM project/data & codes/data_24hz_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_df\n",
    "scaler = StandardScaler()\n",
    "np_scaled = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)\n",
    "\n",
    "data_test = new_df_test\n",
    "scaler = StandardScaler()\n",
    "np_scaled1 = scaler.fit_transform(data_test)\n",
    "data_test = pd.DataFrame(np_scaled1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1677, 2560), (594, 2560))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(new_df)\n",
    "a = data\n",
    "b = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1677, 2560)\n",
      "(594, 2560)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = a#.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2550</th>\n",
       "      <th>2551</th>\n",
       "      <th>2552</th>\n",
       "      <th>2553</th>\n",
       "      <th>2554</th>\n",
       "      <th>2555</th>\n",
       "      <th>2556</th>\n",
       "      <th>2557</th>\n",
       "      <th>2558</th>\n",
       "      <th>2559</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.065564</td>\n",
       "      <td>0.085626</td>\n",
       "      <td>0.105892</td>\n",
       "      <td>0.041316</td>\n",
       "      <td>0.097256</td>\n",
       "      <td>0.138798</td>\n",
       "      <td>0.116977</td>\n",
       "      <td>0.099296</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>0.123161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153676</td>\n",
       "      <td>0.094256</td>\n",
       "      <td>0.094986</td>\n",
       "      <td>0.140133</td>\n",
       "      <td>0.098431</td>\n",
       "      <td>0.065177</td>\n",
       "      <td>0.174835</td>\n",
       "      <td>0.065951</td>\n",
       "      <td>0.069434</td>\n",
       "      <td>0.071726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>0.031701</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>-0.086902</td>\n",
       "      <td>-0.067060</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>-0.007409</td>\n",
       "      <td>-0.110176</td>\n",
       "      <td>0.027574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266763</td>\n",
       "      <td>0.574995</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>-0.391976</td>\n",
       "      <td>-0.569916</td>\n",
       "      <td>0.203095</td>\n",
       "      <td>0.597526</td>\n",
       "      <td>0.150933</td>\n",
       "      <td>-0.471061</td>\n",
       "      <td>-0.598782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.214658</td>\n",
       "      <td>-0.260777</td>\n",
       "      <td>-0.120977</td>\n",
       "      <td>-0.142930</td>\n",
       "      <td>0.113547</td>\n",
       "      <td>0.102680</td>\n",
       "      <td>0.135541</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.080289</td>\n",
       "      <td>-0.072308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546820</td>\n",
       "      <td>-0.620742</td>\n",
       "      <td>0.268237</td>\n",
       "      <td>0.906496</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>-0.643541</td>\n",
       "      <td>-0.768522</td>\n",
       "      <td>-0.325921</td>\n",
       "      <td>0.810754</td>\n",
       "      <td>0.559382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019597</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.088870</td>\n",
       "      <td>-0.122528</td>\n",
       "      <td>0.040385</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>-0.088483</td>\n",
       "      <td>-0.130782</td>\n",
       "      <td>-0.133989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071214</td>\n",
       "      <td>-0.076331</td>\n",
       "      <td>-0.062423</td>\n",
       "      <td>0.185912</td>\n",
       "      <td>-0.027158</td>\n",
       "      <td>0.072129</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>-0.263614</td>\n",
       "      <td>0.063371</td>\n",
       "      <td>-0.220232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.485380</td>\n",
       "      <td>0.055102</td>\n",
       "      <td>0.433022</td>\n",
       "      <td>0.546448</td>\n",
       "      <td>-0.235603</td>\n",
       "      <td>-0.285302</td>\n",
       "      <td>-0.128351</td>\n",
       "      <td>0.297995</td>\n",
       "      <td>0.697154</td>\n",
       "      <td>-0.151004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.406218</td>\n",
       "      <td>-0.437458</td>\n",
       "      <td>0.305361</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>-0.251616</td>\n",
       "      <td>-0.296906</td>\n",
       "      <td>-0.026485</td>\n",
       "      <td>0.459205</td>\n",
       "      <td>0.517611</td>\n",
       "      <td>-0.596410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1672</td>\n",
       "      <td>0.048657</td>\n",
       "      <td>0.346393</td>\n",
       "      <td>-0.186402</td>\n",
       "      <td>-0.165820</td>\n",
       "      <td>0.612012</td>\n",
       "      <td>-0.584169</td>\n",
       "      <td>-0.586134</td>\n",
       "      <td>-0.046100</td>\n",
       "      <td>0.419875</td>\n",
       "      <td>0.295493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168008</td>\n",
       "      <td>-0.288835</td>\n",
       "      <td>0.316212</td>\n",
       "      <td>-0.543888</td>\n",
       "      <td>-0.050714</td>\n",
       "      <td>0.494334</td>\n",
       "      <td>-0.309451</td>\n",
       "      <td>-0.039374</td>\n",
       "      <td>0.498856</td>\n",
       "      <td>-0.292609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1673</td>\n",
       "      <td>0.235414</td>\n",
       "      <td>0.202877</td>\n",
       "      <td>-0.729659</td>\n",
       "      <td>-0.444057</td>\n",
       "      <td>-0.180762</td>\n",
       "      <td>-0.285311</td>\n",
       "      <td>-0.726043</td>\n",
       "      <td>-1.218450</td>\n",
       "      <td>-0.547758</td>\n",
       "      <td>0.023415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137198</td>\n",
       "      <td>-0.084018</td>\n",
       "      <td>-0.079268</td>\n",
       "      <td>0.042475</td>\n",
       "      <td>0.097337</td>\n",
       "      <td>-0.068028</td>\n",
       "      <td>-0.059309</td>\n",
       "      <td>-0.090135</td>\n",
       "      <td>-0.041069</td>\n",
       "      <td>0.048834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1674</td>\n",
       "      <td>0.133432</td>\n",
       "      <td>-0.322978</td>\n",
       "      <td>-0.152847</td>\n",
       "      <td>0.535306</td>\n",
       "      <td>0.247962</td>\n",
       "      <td>-0.483001</td>\n",
       "      <td>-1.074762</td>\n",
       "      <td>-0.224274</td>\n",
       "      <td>0.513402</td>\n",
       "      <td>0.281206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112234</td>\n",
       "      <td>-0.029860</td>\n",
       "      <td>-0.068976</td>\n",
       "      <td>0.122265</td>\n",
       "      <td>-0.004491</td>\n",
       "      <td>-0.201401</td>\n",
       "      <td>-0.136434</td>\n",
       "      <td>-0.399055</td>\n",
       "      <td>-0.208644</td>\n",
       "      <td>0.101730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.060180</td>\n",
       "      <td>-0.122849</td>\n",
       "      <td>-0.084911</td>\n",
       "      <td>-0.046460</td>\n",
       "      <td>-0.360388</td>\n",
       "      <td>-0.107335</td>\n",
       "      <td>-0.253156</td>\n",
       "      <td>-0.207768</td>\n",
       "      <td>-0.109628</td>\n",
       "      <td>-0.440717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141732</td>\n",
       "      <td>-0.128188</td>\n",
       "      <td>-0.133450</td>\n",
       "      <td>-0.044162</td>\n",
       "      <td>-0.118532</td>\n",
       "      <td>-0.150986</td>\n",
       "      <td>-0.066113</td>\n",
       "      <td>-0.142165</td>\n",
       "      <td>-0.212651</td>\n",
       "      <td>-0.073792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1676</td>\n",
       "      <td>0.170405</td>\n",
       "      <td>0.186763</td>\n",
       "      <td>0.205279</td>\n",
       "      <td>0.137503</td>\n",
       "      <td>0.197541</td>\n",
       "      <td>0.219473</td>\n",
       "      <td>0.203569</td>\n",
       "      <td>0.173785</td>\n",
       "      <td>0.178012</td>\n",
       "      <td>0.207767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269240</td>\n",
       "      <td>0.272836</td>\n",
       "      <td>0.258196</td>\n",
       "      <td>0.344257</td>\n",
       "      <td>0.236829</td>\n",
       "      <td>0.275069</td>\n",
       "      <td>0.304936</td>\n",
       "      <td>0.259819</td>\n",
       "      <td>0.272054</td>\n",
       "      <td>0.281527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1677 rows Ã— 2560 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.065564  0.085626  0.105892  0.041316  0.097256  0.138798  0.116977   \n",
       "1     0.030134  0.031701  0.028579 -0.086902 -0.067060 -0.000755  0.002007   \n",
       "2    -0.214658 -0.260777 -0.120977 -0.142930  0.113547  0.102680  0.135541   \n",
       "3     0.019597  0.026566  0.088870 -0.122528  0.040385  0.013703  0.060000   \n",
       "4    -0.485380  0.055102  0.433022  0.546448 -0.235603 -0.285302 -0.128351   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1672  0.048657  0.346393 -0.186402 -0.165820  0.612012 -0.584169 -0.586134   \n",
       "1673  0.235414  0.202877 -0.729659 -0.444057 -0.180762 -0.285311 -0.726043   \n",
       "1674  0.133432 -0.322978 -0.152847  0.535306  0.247962 -0.483001 -1.074762   \n",
       "1675  0.060180 -0.122849 -0.084911 -0.046460 -0.360388 -0.107335 -0.253156   \n",
       "1676  0.170405  0.186763  0.205279  0.137503  0.197541  0.219473  0.203569   \n",
       "\n",
       "          7         8         9     ...      2550      2551      2552  \\\n",
       "0     0.099296  0.066929  0.123161  ...  0.153676  0.094256  0.094986   \n",
       "1    -0.007409 -0.110176  0.027574  ...  0.266763  0.574995  0.005676   \n",
       "2     0.028200  0.080289 -0.072308  ... -0.546820 -0.620742  0.268237   \n",
       "3    -0.088483 -0.130782 -0.133989  ... -0.071214 -0.076331 -0.062423   \n",
       "4     0.297995  0.697154 -0.151004  ... -0.406218 -0.437458  0.305361   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1672 -0.046100  0.419875  0.295493  ... -0.168008 -0.288835  0.316212   \n",
       "1673 -1.218450 -0.547758  0.023415  ... -0.137198 -0.084018 -0.079268   \n",
       "1674 -0.224274  0.513402  0.281206  ... -0.112234 -0.029860 -0.068976   \n",
       "1675 -0.207768 -0.109628 -0.440717  ... -0.141732 -0.128188 -0.133450   \n",
       "1676  0.173785  0.178012  0.207767  ...  0.269240  0.272836  0.258196   \n",
       "\n",
       "          2553      2554      2555      2556      2557      2558      2559  \n",
       "0     0.140133  0.098431  0.065177  0.174835  0.065951  0.069434  0.071726  \n",
       "1    -0.391976 -0.569916  0.203095  0.597526  0.150933 -0.471061 -0.598782  \n",
       "2     0.906496  0.218579 -0.643541 -0.768522 -0.325921  0.810754  0.559382  \n",
       "3     0.185912 -0.027158  0.072129  0.021281 -0.263614  0.063371 -0.220232  \n",
       "4     0.710938 -0.251616 -0.296906 -0.026485  0.459205  0.517611 -0.596410  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1672 -0.543888 -0.050714  0.494334 -0.309451 -0.039374  0.498856 -0.292609  \n",
       "1673  0.042475  0.097337 -0.068028 -0.059309 -0.090135 -0.041069  0.048834  \n",
       "1674  0.122265 -0.004491 -0.201401 -0.136434 -0.399055 -0.208644  0.101730  \n",
       "1675 -0.044162 -0.118532 -0.150986 -0.066113 -0.142165 -0.212651 -0.073792  \n",
       "1676  0.344257  0.236829  0.275069  0.304936  0.259819  0.272054  0.281527  \n",
       "\n",
       "[1677 rows x 2560 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = b#.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1677, 2560)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape\n",
    "#type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = aa.shape[1]\n",
    "input_shape = (original_dim, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dropout, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.losses import mse, binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE** = encoder + decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q(z|X) -- encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(30, activation='relu')(inputs) #bich_ka_dim =100\n",
    "z_mean = Dense(2, name='z_mean')(x) #latent_dim= 2\n",
    "z_log_var = Dense(2, name='z_log_var')(x) #latent_dim=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the parameters above to sample new points from latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample z ~ Q(z|X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim)) #mean 0 var 1, by def\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "z = Lambda(sampling, output_shape=(2,), name='z')([z_mean, z_log_var]) #latent_dim=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                76830     \n",
      "_________________________________________________________________\n",
      "z_mean (Dense)               (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 76,892\n",
      "Trainable params: 76,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Model(inputs, z_mean) #instantiate\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P(X|z) -- decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30)                90        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2560)              79360     \n",
      "=================================================================\n",
      "Total params: 79,450\n",
      "Trainable params: 79,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = Input(shape=(2,), name='z_sampling') #latent_dim=2\n",
    "x = Dense(30, activation='relu')(latent_inputs) #bich_dim =100\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# Instantiate the decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the VAE model\n",
    "outputs = decoder(encoder(inputs))\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = mse(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1677 samples, validate on 594 samples\n",
      "Epoch 1/10\n",
      "1677/1677 [==============================] - 0s 280us/step - loss: 445719.8353 - val_loss: 18.5033\n",
      "Epoch 2/10\n",
      "1677/1677 [==============================] - 0s 103us/step - loss: 1080757.7035 - val_loss: 14.2034\n",
      "Epoch 3/10\n",
      "1677/1677 [==============================] - 0s 95us/step - loss: 252239.8745 - val_loss: 11.5140\n",
      "Epoch 4/10\n",
      "1677/1677 [==============================] - 0s 94us/step - loss: 36.1214 - val_loss: 9.1988\n",
      "Epoch 5/10\n",
      "1677/1677 [==============================] - 0s 94us/step - loss: 89.8629 - val_loss: 7.1263\n",
      "Epoch 6/10\n",
      "1677/1677 [==============================] - 0s 93us/step - loss: 4.1372 - val_loss: 4.9311\n",
      "Epoch 7/10\n",
      "1677/1677 [==============================] - 0s 93us/step - loss: 3.2619 - val_loss: 3.6160\n",
      "Epoch 8/10\n",
      "1677/1677 [==============================] - 0s 94us/step - loss: 2.9275 - val_loss: 2.9272\n",
      "Epoch 9/10\n",
      "1677/1677 [==============================] - 0s 103us/step - loss: 2.6435 - val_loss: 2.7389\n",
      "Epoch 10/10\n",
      "1677/1677 [==============================] - 0s 91us/step - loss: 2.3699 - val_loss: 2.9004\n"
     ]
    }
   ],
   "source": [
    "results = vae.fit(aa, aa,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=128,\n",
    "        validation_data=(bb, bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the loss distribution of the training set\n",
    "X_pred = vae.predict(aa)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[1])\n",
    "X_pred = pd.DataFrame(X_pred, columns=a.columns)\n",
    "X_pred.index = aa.index\n",
    "\n",
    "scored = pd.DataFrame(index=a.index)\n",
    "#Xtrain = aa.reshape(aa.shape[0], aa.shape[1])\n",
    "scored['Loss_mse'] = np.mean(np.square(X_pred-aa), axis = 1)\n",
    "plt.figure(figsize=(16,9), dpi=80)\n",
    "plt.title('Loss Distribution for mse', fontsize=16)\n",
    "sns.distplot(scored['Loss_mse'], bins = 20, kde= True, color = 'blue');\n",
    "plt.xlim([0.0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred = vae.predict(aa)\n",
    "\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - aa), axis=1)\n",
    "train_mse_loss = np.mean(np.square(X_train_pred - aa), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_mae_loss, train_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = vae.predict(bb)\n",
    "\n",
    "test_mae_loss = np.mean(np.abs(X_pred - bb), axis=1)\n",
    "test_mse_loss = np.mean(np.square(X_pred - bb), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0      0.017422\n",
       " 1      0.210200\n",
       " 2      0.015382\n",
       " 3      0.016023\n",
       " 4      0.757381\n",
       "          ...   \n",
       " 589    1.898046\n",
       " 590    1.280087\n",
       " 591    0.020028\n",
       " 592    0.015735\n",
       " 593    0.070433\n",
       " Length: 594, dtype: float64, 0      0.000422\n",
       " 1      0.049076\n",
       " 2      0.000332\n",
       " 3      0.000362\n",
       " 4      0.655587\n",
       "          ...   \n",
       " 589    3.603742\n",
       " 590    1.638850\n",
       " 591    0.000610\n",
       " 592    0.000360\n",
       " 593    0.006568\n",
       " Length: 594, dtype: float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae_loss, test_mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS = test_mse_loss > 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1       True\n",
       "2      False\n",
       "3      False\n",
       "4       True\n",
       "       ...  \n",
       "589     True\n",
       "590     True\n",
       "591    False\n",
       "592    False\n",
       "593     True\n",
       "Length: 594, dtype: bool"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MS_answer = np.any(MS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(MS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"/Users/aj/docs/useful/sem3/MLDM project/data & codes/labels.csv\", sep=\",\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number_list = [i for i in range(594)]   #sample number list\n",
    "header = ['seqID', 'anomaly'] \n",
    "import csv\n",
    "\n",
    "with open('will_upload_1.csv', 'w', newline='') as f:\n",
    "    w = csv.writer(f, delimiter=';')\n",
    "    w.writerow(header)\n",
    "    for row in zip(sample_number_list, MS):\n",
    "        w.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = pd.read_csv(\"will_upload_1.csv\", sep=\";\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: TP=254, FP=0, FN=43, TN=297\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "true_negative = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "for i in range(594):\n",
    "    if labels.iloc[i][0] == myfile.iloc[i][0]:\n",
    "        if labels.iloc[i][0] == 1:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            true_negative += 1\n",
    "    else:\n",
    "        if labels.iloc[i][0] == 1:\n",
    "            false_negative += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "print(\"Results: TP={0}, FP={1}, FN={2}, TN={3}\".format(true_positive, false_positive, false_negative, true_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297, 297)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count1=0\n",
    "count0 = 0\n",
    "for i in range(594):\n",
    "    if labels.iloc[i][0]==1:\n",
    "        count1 = count1 + 1\n",
    "    else:\n",
    "        count0 = count0 +1\n",
    "count1, count0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: F1=0.92196, Precision=1.0, Recall=0.85522\n"
     ]
    }
   ],
   "source": [
    "precision = round(true_positive/(true_positive + false_positive),5)\n",
    "recall =  round(true_positive/(true_positive + false_negative),5)\n",
    "F1 =  round(((recall*precision)/(recall + precision))*2,5)\n",
    "print(\"Results: F1={0}, Precision={1}, Recall={2}\".format(F1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
